{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from global_variables import DIM\n",
    "\n",
    "from simulation import read_results, read_params\n",
    "from global_variables import DIM\n",
    "from plotting_tools import add_plot_decoration, generate_labels\n",
    "from matplotlib.colors import LogNorm\n",
    "from plotting_tools import plot_noise\n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise analysis\n",
    "\n",
    "In order to analyse algorythms robustness to noise we have to know the scale of measrurements. \n",
    "We can see on the plot below, that in the current setup, the distances have a multimodal distribution that is looks like a mixture of gaussians (with the difference that it's only positive, or that the bigger gaussian is squed). The distance wary between $0.1$ and $25$.\n",
    "\n",
    "For analysis of real distances, see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'get_distances'\n",
    "save_figures = True\n",
    "\n",
    "resultfolder = 'results/{}/'.format(key)\n",
    "results = read_results(resultfolder + 'result_')\n",
    "#parameters = read_params(resultfolder + 'parameters.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 5\n",
    "\n",
    "ist, bins, _ = plt.hist(np.sqrt(results['distances']), bins=100)\n",
    "if save_figures:\n",
    "    plt.savefig(resultfolder + \"simulated_distances.pdf\", bbox_inches=\"tight\")\n",
    "plt.title(\"simulated distance distribution\")\n",
    "plt.show()\n",
    "logbins = np.logspace(np.log10(bins[0]),np.log10(bins[-1]),len(bins))\n",
    "plt.hist(np.sqrt(results['distances']), bins=logbins)\n",
    "plt.xscale('log')\n",
    "plt.title(\"simulated distance distribution (log scale)\")\n",
    "if save_figures:\n",
    "    plt.savefig(resultfolder + \"simulated_distances_log.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real distance distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate_dataset import read_anchors_df, read_dataset\n",
    "\n",
    "names =  ['circle2_double.csv', \n",
    "             'circle3_triple.csv', \n",
    "             'clover.csv',\n",
    "             'eight2_double.csv', \n",
    "             'rounds.csv', \n",
    "             'straight1.csv', \n",
    "             'straight2.csv', \n",
    "             'straight3.csv', \n",
    "             'straight4.csv', \n",
    "             'straight5.csv', \n",
    "             'straight6.csv', \n",
    "             'triangle_double.csv']\n",
    "\n",
    "anchorsfile = 'experiments/anchors.csv'\n",
    "anchors_df = read_anchors_df(anchorsfile)\n",
    "anchors = anchors_df[[\"px\", \"py\", \"pz\"]].values.T\n",
    "\n",
    "noisy_distances = []\n",
    "\n",
    "for name in names:\n",
    "    datafile = 'experiments/robot_test/' + name\n",
    "    data_df = read_dataset(datafile, anchors_df)\n",
    "    noisy_distances.extend(data_df[data_df.system_id==\"RTT\"][\"distance\"].values.tolist())\n",
    "np.savetxt(resultfolder + 'noisy_distances.csv', np.array(noisy_distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from trajectory import Trajectory\n",
    "from measurements import get_D_topright\n",
    "\n",
    "MM = 0.001\n",
    "\n",
    "with open('controls/robot_trajectores.pkl', 'rb') as f:\n",
    "    trajectories = pickle.load(f)\n",
    "\n",
    "distances = []\n",
    "n_samples = 400\n",
    "    \n",
    "for traj in trajectories:\n",
    "    trajectory = Trajectory(**traj['parameters'])\n",
    "    # Dont strech trajectories that have hand designed coefficients\n",
    "    trajectory.scale_bounding_box(traj['box'] * MM, keep_aspect_ratio='coeffs' in traj['parameters'])\n",
    "    trajectory.center()\n",
    "    basis = trajectory.get_basis(n_samples=n_samples)\n",
    "    positions = trajectory.get_sampling_points(basis)\n",
    "    positions = np.concatenate([positions, np.zeros((1, n_samples))])\n",
    "    distances_squared = get_D_topright(anchors=anchors, samples=positions)\n",
    "    distances.extend(np.sqrt(distances_squared.flatten()).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pair(dist_dict, save_figures=False):\n",
    "    \"\"\"\n",
    "    Plot distance histograms in normal and log scale. \n",
    "    \n",
    "    :param dist_dict: dictionary with either {label:distances} or {label:filename}, \n",
    "                      where distances is the list of simulated distances (see cell above),\n",
    "                      or filename is the name of the txtfile where data was stored (see 2 cells above).\n",
    "    \n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(2, 1)\n",
    "    fig.set_size_inches(10, 7)\n",
    "    \n",
    "    axs[0].set_title(\"distance distribution\")\n",
    "    axs[1].set_title(\"noisy distance distribution (log scale)\")\n",
    "    \n",
    "    bins = np.linspace(0.5, 20, 100)\n",
    "    logbins = np.logspace(np.log10(bins[0]),np.log10(bins[-1]),len(bins))\n",
    "    \n",
    "    for label, distances in dist_dict.items():\n",
    "        if type(distances) == str: # distances is just a file name.\n",
    "            distances = np.loadtxt(resultfolder + distances)\n",
    "            \n",
    "        axs[0].hist(distances, bins=bins, label=label, alpha=0.7)\n",
    "        axs[1].hist(distances, bins=logbins, label=label, alpha=0.7)\n",
    "        axs[1].set_xscale('log')\n",
    "        \n",
    "    if save_figures:\n",
    "        plt.savefig(resultfolder + \"distances.pdf\", bbox_inches=\"tight\")\n",
    "            \n",
    "    axs[0].legend()\n",
    "    axs[1].legend()\n",
    "    return fig\n",
    "\n",
    "fig = plot_pair({\"noisy\":\"noisy_distances.csv\", \"simulated\":distances})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see real distance distribution. Note that the true distances are not realy the true distances, but a simulation where the trajectory is in the middle of the room. I think for our pourposes it's enough. Might be good to calibrate the noisy distances before plotting them here (as it's what we will be using for reconstruction).\n",
    "\n",
    "We can see at the plots below, that the noisy distances (without callibration) range between $1$ and $20$ meters, where $20$ meters is clearly and outlier. The shape of the disriburion seems to be unimodal (which is good, in general), and match the shape of the bigger of the simulated modes. The true distances fit nicely between $1$ and $8$, and are (more or less) unimodal. The log plot looks shifted, which would suggest that noisy distances are multiplied by some factor (again, maybe I should calibrate distances before plotting).\n",
    "\n",
    "It seems to me that the simulated distribution is more difficult to work with than the real one (which means that if algorytms below work for simulations, they will work for true data). The only issue is that we don't know the noise distribution of the real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibrated distance distribution\n",
    "\n",
    "In below cells we plot calibrated vs. tango vs. original (\"noisy\") distance measurements. We see that calibration corrects the offset that we saw in above plots. We also see that the Michalina's hack to quickly calculate distances from the trajectory by centering it is reasonably close to the actual distances we get from tango."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "calib_distances = []\n",
    "tango_distances = []\n",
    "\n",
    "for name in names:\n",
    "    calib_name = name.replace('.csv', '_calibrated.pkl')\n",
    "    datafile = 'experiments/robot_test/' + calib_name\n",
    "    data_df = pd.read_pickle(datafile)\n",
    "    print('read', datafile)\n",
    "    calib_distances.extend(data_df[data_df.system_id==\"RTT\"][\"distance_median_0\"].values.tolist())\n",
    "    tango_distances.extend(data_df[data_df.system_id==\"RTT\"][\"distance_tango\"].values.tolist())\n",
    "np.savetxt(resultfolder + 'calib_distances.csv', np.array(calib_distances))\n",
    "np.savetxt(resultfolder + 'tango_distances.csv', np.array(tango_distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_pair({\"tango\":\"tango_distances.csv\", \n",
    "           \"calib\":\"calib_distances.csv\", \n",
    "           \"noisy\":\"noisy_distances.csv\"})\n",
    "fig.suptitle(\"Comparing non-calibrated to calibrated distances.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_pair({\"tango\":\"tango_distances.csv\", \n",
    "                 \"simulated\":distances})\n",
    "fig.suptitle(\"Comparing simulated 'real' distances to Tango data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise added to distances, basic right inverse\n",
    "In this case, the noise is added to measurements, what leads to adding `distance x noise` to squared distances (assuming that we can neglect $\\sigma^2$). If we use a version of OLS (right inverse), we assume the noise to be gaussian. This might lead to bigger errors. \n",
    "\n",
    "Below, we can see reconstruction errors for different number of available measurements and different noise $\\sigma$s. We consider noise up to $\\sigma = 10$, because that is still within the range of distance measurements. \n",
    "\n",
    "We plot three different errors - relative and absolute distance errors, and coefficient reconstruction errors. The distance errors is what is more intuitive. We should think what metric we should report. \n",
    "\n",
    "We can see that there is huge difference on betwen absolute and relative errors, which suggest large errors in reconstruction of large distances. Or maybe it's just because we take a mean and not meadian error. \n",
    "\n",
    "We can see that with oversampling the error drops more or less linearly on the log-log plot, with $<10\\times$ oversampling leads to more than $10\\times$ decrease in error. However, some of this decrease (see the nonlinear behaviour of the plots for small number of samples) comes from the rank deficiency, which is more likely for small number of samples. We can see roguhly the same drop on all plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 10, 5\n",
    "plot_noise('noise_right_inverse', save_figures=True, max_noise=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise added to distances, weighted right inverse\n",
    "In this case, the noise is added to measurements, what leads to adding `distance x noise` to squared distances (assuming we can neglect $\\sigma^2$). We can try to normalize the error, by dividing by the (noisy) distance. I divide by the noisy distance $+10^{-3}$, to avoid dividing by really small distances.\n",
    "\n",
    "The results are more or less the same as for OLS, but it seems that the improvement with oversampling is sligthly bigger, with better errors on coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_noise('noise_right_inverse_weighted', save_figures=True,  max_noise=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise added to squared distances\n",
    "In this case the model matches OLS assumptions, so there is no need to use WLS. It's mostly for reference, to see if there is any strange behaviour that comes from trajectory model and not the noise model.\n",
    "\n",
    "We can see some outliers for large noise, but that could go away with more simulations. Overall, the behaviour is similar than the previous plots, with maybe $10\\times$ smaller relative and coefficient errors. Seems that the squared noise is not a big issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_noise('noise_to_square_right_inverse', save_figures=True,  max_noise=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
