{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from global_variables import DIM\n",
    "\n",
    "from simulation import read_results, read_params\n",
    "from global_variables import DIM\n",
    "from plotting_tools import add_plot_decoration, generate_labels\n",
    "from matplotlib.colors import LogNorm\n",
    "from plotting_tools import plot_noise\n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise analysis\n",
    "\n",
    "In order to analyse algorythms robustness to noise we have to know the scale of measrurements. \n",
    "We can see on the plot below, that in the current setup, the distances have a multimodal distribution that is looks like a mixture of gaussians (with the difference that it's only positive, or that the bigger gaussian is squed). The distance wary between $0.1$ and $25$.\n",
    "\n",
    "For analysis of real distances, see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'get_distances'\n",
    "save_figures = True\n",
    "\n",
    "resultfolder = 'results/{}/'.format(key)\n",
    "results = read_results(resultfolder + 'result_')\n",
    "parameters = read_params(resultfolder + 'parameters.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 5\n",
    "\n",
    "ist, bins, _ = plt.hist(np.sqrt(results['distances']), bins=100)\n",
    "if save_figures:\n",
    "    plt.savefig(resultfolder + \"simulated_distances.pdf\", bbox_inches=\"tight\")\n",
    "plt.title(\"simulated distance distribution\")\n",
    "plt.show()\n",
    "logbins = np.logspace(np.log10(bins[0]),np.log10(bins[-1]),len(bins))\n",
    "plt.hist(np.sqrt(results['distances']), bins=logbins)\n",
    "plt.xscale('log')\n",
    "plt.title(\"simulated distance distribution (log scale)\")\n",
    "if save_figures:\n",
    "    plt.savefig(resultfolder + \"simulated_distances_log.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real distance distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate_dataset import read_anchors_df, read_dataset\n",
    "\n",
    "names =  ['circle2_double.csv', \n",
    "             'circle3_triple.csv', \n",
    "             'clover.csv',\n",
    "             'eight2_double.csv', \n",
    "             'rounds.csv', \n",
    "             'straight1.csv', \n",
    "             'straight2.csv', \n",
    "             'straight3.csv', \n",
    "             'straight4.csv', \n",
    "             'straight5.csv', \n",
    "             'straight6.csv', \n",
    "             'triangle_double.csv']\n",
    "\n",
    "anchorsfile = 'experiments/anchors.csv'\n",
    "anchors_df = read_anchors_df(anchorsfile)\n",
    "anchors = anchors_df[[\"px\", \"py\", \"pz\"]].values.T\n",
    "\n",
    "noisy_distances = []\n",
    "\n",
    "for name in names:\n",
    "    datafile = 'experiments/robot_test/' + name\n",
    "    data_df = read_dataset(datafile, anchors_df)\n",
    "    noisy_distances.extend(data_df[data_df.system_id==\"RTT\"][\"distance\"].values.tolist())\n",
    "np.savetxt(resultfolder + 'noisy_distances.csv', np.array(noisy_distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from trajectory import Trajectory\n",
    "from measurements import get_D_topright\n",
    "\n",
    "MM = 0.001\n",
    "\n",
    "with open('controls/robot_trajectores.pkl', 'rb') as f:\n",
    "    trajectories = pickle.load(f)\n",
    "\n",
    "distances = []\n",
    "n_samples = 400\n",
    "    \n",
    "for traj in trajectories:\n",
    "    trajectory = Trajectory(**traj['parameters'])\n",
    "    # Dont strech trajectories that have hand designed coefficients\n",
    "    trajectory.scale_bounding_box(traj['box'] * MM, keep_aspect_ratio='coeffs' in traj['parameters'])\n",
    "    trajectory.center()\n",
    "    basis = trajectory.get_basis(n_samples=n_samples)\n",
    "    positions = trajectory.get_sampling_points(basis)\n",
    "    positions = np.concatenate([positions, np.zeros((1, n_samples))])\n",
    "    distances_squared = get_D_topright(anchors=anchors, samples=positions)\n",
    "    distances.extend(np.sqrt(distances_squared.flatten()).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_distances = np.loadtxt(resultfolder + \"noisy_distances.csv\")\n",
    "bins = np.linspace(0.5, 20, 100)\n",
    "plt.hist(distances, bins=bins, label=\"true distances\", alpha=0.7)\n",
    "plt.hist(noisy_distances, bins=bins, label=\"noisy distances\", alpha=0.7)\n",
    "plt.title(\"distance distribution\")\n",
    "plt.legend()\n",
    "if save_figures:\n",
    "    plt.savefig(resultfolder + \"distances.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "logbins = np.logspace(np.log10(bins[0]),np.log10(bins[-1]),len(bins))\n",
    "plt.hist(distances, bins=logbins, label=\"true distances\", alpha=0.7)\n",
    "plt.hist(noisy_distances, bins=logbins, label=\"noisy distances\", alpha=0.7)\n",
    "plt.xscale('log')\n",
    "plt.title(\"noisy distance distribution (log scale)\")\n",
    "plt.legend()\n",
    "if save_figures:\n",
    "    plt.savefig(resultfolder + \"distances_log.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see real distance distribution. Note that the true distances are not realy the true distances, but a simulation where the trajectory is in the middle of the room. I think for our pourposes it's enough. Might be good to calibrate the noisy distances before plotting them here (as it's what we will be using for reconstruction).\n",
    "\n",
    "We can see at the plots below, that the noisy distances (without callibration) range between $1$ and $20$ meters, where $20$ meters is clearly and outlier. The shape of the disriburion seems to be unimodal (which is good, in general), and match the shape of the bigger of the simulated modes. The true distances fit nicely between $1$ and $8$, and are (more or less) unimodal. The log plot looks shifted, which would suggest that noisy distances are multiplied by some factor (again, maybe I should calibrate distances before plotting).\n",
    "\n",
    "It seems to me that the simulated distribution is more difficult to work with than the real one (which means that if algorytms below work for simulations, they will work for true data). The only issue is that we don't know the noise distribution of the real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise added to distances, basic right inverse\n",
    "In this case, the noise is added to measurements, what leads to adding `distance x noise` to squared distances (assuming that we can neglect $\\sigma^2$). If we use a version of OLS (right inverse), we assume the noise to be gaussian. This might lead to bigger errors. \n",
    "\n",
    "Below, we can see reconstruction errors for different number of available measurements and different noise $\\sigma$s. For large noise $\\sigma = 100$, we can see that for small number of measurements the error is larger than the noise but with oversampling drops to below $10$. It seems that $10\\times$ more samples leads to $10$-fold decrease in error.\n",
    "\n",
    "Notably, the relative error and absolute error have similar values, with relative error having higher variance. The reported errors are on the **coefficients** not distances. **TODO distance errors?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 10, 5\n",
    "plot_noise('noise_right_inverse', save_figures=True, noise_index=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise added to distances, weighted right inverse\n",
    "In this case, the noise is added to measurements, what leads to adding `distance x noise` to squared distances (assuming we can neglect $\\sigma^2$). We can try to normalize the error, by dividing by the (noisy) distance. I divide by the noisy distance $+10^{-3}$, to avoid dividing by really small distances.\n",
    "\n",
    "Below, we can see reconstruction errors for different number of available measurements and different noise $\\sigma$s. For large noise $\\sigma = 100$, we can see that for small number of measurements the error is larger than the noise $10^{3}$ but with oversampling drops to below $1$.  It seems that $10\\times$ more samples leads to almost $100$-fold decrease in error, **better than OLS**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_noise('noise_right_inverse_weighted', save_figures=True, noise_index=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise added to squared distances\n",
    "In this case the model matches OLS assumptions, so there is no need to use WLS. Here we have, however, a strange behaviour for small number of measurements and large noise. Note however, that additive noise with $\\sigma = 100$ is rather large, so we don't need worry about it. For other noise variances, the error decreases roughly linearly with number of measurements, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_noise('noise_to_square_right_inverse', save_figures=True, noise_index=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
